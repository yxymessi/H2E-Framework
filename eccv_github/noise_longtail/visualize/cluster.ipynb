{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image, ImageDraw\n",
    "from io import BytesIO\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import feature as skif\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "torch.cuda.set_device(1)\n",
    "random.seed(25)\n",
    "\n",
    "DATA_PATH = './data/dataset/mini-imagenet/new_setting/red_noise_nl_0.1/'\n",
    "SUP_FEATURE_PATH = '/data/features_sup/{}.torch'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "class ImageNetSolo(Dataset):\n",
    "    def __init__(self, phase, index, data_path):\n",
    "        super(ImageNetSolo, self).__init__()\n",
    "        categories = os.listdir(data_path)\n",
    "        names = os.listdir(os.path.join(data_path, categories[index]))\n",
    "        self.category = categories[index]\n",
    "        self.images = [os.path.join(DATA_PATH, categories[index], name) for name in names]\n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.Resize(270),\n",
    "                            transforms.CenterCrop(256),\n",
    "                            transforms.ToTensor(),])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.images[index]\n",
    "        with open(path, 'rb') as f:\n",
    "            sample = Image.open(f).convert('RGB')\n",
    "            sample = self.transform(sample)\n",
    "        return sample, index\n",
    "    \n",
    "def get_loader(phase, index, batch_size=256):\n",
    "    dataset = ImageNetSolo(phase, index, DATA_PATH)\n",
    "    loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True if phase == 'train' else False, \n",
    "                             pin_memory=True, num_workers=8)\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(model, index):\n",
    "    # show reconstructed image\n",
    "    indexes = []\n",
    "    features = []\n",
    "    out_paths = []\n",
    "    model.eval()\n",
    "    val_loader, val_data = get_loader(phase='val', index=index, batch_size=32)\n",
    "    img_paths = val_data.images\n",
    "    category = val_data.category\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, ids in val_loader:\n",
    "            images = images.cuda()\n",
    "            z = model(images)\n",
    "            indexes.append(ids.view(-1).cpu())\n",
    "            features.append(z.cpu())\n",
    "        indexes = torch.cat(indexes, dim=0).numpy()\n",
    "        features = torch.cat(features, dim=0).numpy()\n",
    "    \n",
    "    for idx in list(indexes):\n",
    "        out_paths.append(img_paths[idx])\n",
    "    return features, out_paths, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_label(features, category, out_paths, output_dict, NUM_CLUSTER=6):\n",
    "    output_dict[category] = {}\n",
    "    kmeans = KMeans(n_clusters=NUM_CLUSTER).fit(features)\n",
    "    for path, label in zip(out_paths, list(kmeans.labels_)):\n",
    "        if label in output_dict[category]:\n",
    "            output_dict[category][label].append(path)\n",
    "        else:\n",
    "            output_dict[category][label] = [path,]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displays_image_list(path_dict, print_number=6, num_per_line=15):\n",
    "    plt.figure(figsize=(num_per_line*2,print_number*2))\n",
    "    for k in range(print_number):\n",
    "        lines = print_number\n",
    "        for j in range(num_per_line):\n",
    "            idx = k * num_per_line + j\n",
    "            idx_plot = idx\n",
    "            img_temp = Image.open(path_dict[k][j]).resize((64,64))\n",
    "            #Image.fromarray(np.uint8((org_img[intra_label[k][j]])*255))\n",
    "            ax = plt.subplot(lines, num_per_line, idx_plot+1)##控制plot位置\n",
    "            plt.imshow(img_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load moco model\n",
    "model = torchvision.models.resnet50(pretrained=True).cuda()\n",
    "model.fc = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "NUM_CLUSTER = 6\n",
    "for index in range(len(os.listdir(DATA_PATH))):\n",
    "    print('======== Leanring {} category ==========='.format(index))\n",
    "    features, out_paths, category = get_outputs(model, index)\n",
    "    output_dict = get_cluster_label(features, category, out_paths, output_dict, NUM_CLUSTER=NUM_CLUSTER)\n",
    "    # save features and image path\n",
    "    torch.save({'features':features, 'paths':out_paths}, SUP_FEATURE_PATH.format(category))\n",
    "torch.save(output_dict, SUP_FEATURE_PATH.format('clustering_outputs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize results\n",
    "index = 8\n",
    "category = list(output_dict.keys())[index]\n",
    "displays_image_list(output_dict[category], print_number=NUM_CLUSTER, num_per_line=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize distribution\n",
    "count_dict={}\n",
    "for key,value in output_dict.items():\n",
    "    count_dict[key] ={}\n",
    "    count_dict[key]['0'] = len(value[0])\n",
    "    count_dict[key]['1'] = len(value[1])\n",
    "    count_dict[key]['2'] = len(value[2])\n",
    "    count_dict[key]['3'] = len(value[3])\n",
    "    count_dict[key]['4'] = len(value[4])\n",
    "    count_dict[key]['5'] = len(value[5])\n",
    "\n",
    "print(count_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('LWN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "406f4f9549b37ae82acfac2a427e8bacb16ad0662349a2733d23c3b8a370d9fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
